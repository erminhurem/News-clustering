def fetch_news():
    feeds = [
        'https://www.telegraf.rs/rss',
        'https://www.kurir.rs/rss',
        'https://nova.rs/rss',
        'https://n1info.ba/feed/',
        'https://www.euronews.rs/rss',
        'https://informer.rs/rss',
        'https://n1info.ba/rss',
        'https://n1info.hr/rss',
        'https://www.vijesti.ba/rss/svevijesti',
        'https://balkans.aljazeera.net/rss.xml',
        'https://www.klix.ba/rss',
        'https://www.avaz.ba/rss',
        'https://www.oslobodjenje.ba/feed',
        'https://okanal.oslobodjenje.ba/okanal/feed',
        'https://www.b92.net/feed/',
        'https://www.sd.rs/rss.xml',
        'https://www.rts.rs/rss/ci.html',
        'https://www.alo.rs/rss/live',
        'https://www.alo.rs/rss/hronika',
        'https://www.alo.rs/rss/biz',
        'https://www.alo.rs/rss/vesti',
        'https://www.espreso.co.rs/rss',
        'https://www.vecernji.hr/feeds/latest',
        'https://dnevnik.hr/assets/feed/articles',
        'https://www.24sata.hr/feeds/najnovije.xml',
        'https://www.24sata.hr/feeds/news.xml',
        'https://www.24sata.hr/feeds/sport.xml',
        'https://www.24sata.hr/feeds/tech.xml',

    ]
    logger = logging.getLogger(__name__)
    for feed_url in feeds:
        feed = feedparser.parse(feed_url)
        for entry in feed.entries:
            if not Headlines.objects.filter(link=entry.link).exists():
                published_date = None
                if entry.published:
                    try:
                        published_date = date_parser.parse(entry.published)
                    except ValueError:
                        pass

                content = entry.content[0].value if 'content' in entry else ''
                soup = BeautifulSoup(content, 'html.parser')
                images = soup.find_all('img')

                image_urls = [img['src'] for img in images]

                category = categorize_news(entry.link)  # Ovdje dodajemo kategoriju

                news_item = Headlines(
                    title=entry.title,
                    link=entry.link,
                    description=entry.description,
                    published_date=published_date,
                    source=feed_url,
                    category=category, 
                    image_urls=','.join(image_urls) if image_urls else None
                )
                logger.info('Fetching news from %s', feed_url)
                news_item.save() 


def company_directory(request):
    file_paths = [
       
        settings.BASE_DIR / 'static' / 'Baza 2003.xlsx',
    ]
   # Provjeravamo da li fajlovi postoje
    existing_files = [fp for fp in file_paths if Path(fp).exists()]
    if not existing_files:
        # Ako ne postoje fajlovi, vratite odgovarajući kontekst
        context = {'error': 'Nijedan od navedenih fajlova ne postoji.'}
        return render(request, 'firme.html', context)

    try:
        # Pretpostavljamo da funkcija create_company_directory_adjusted pravilno učitava fajlove
        companies_count_by_city = create_company_directory_adjusted(existing_files)
    except Exception as e:
        # Ako dođe do greške prilikom učitavanja, uhvatite je i proslijedite u kontekst
        context = {'error': f'Došlo je do greške prilikom obrade fajlova: {e}'}
        return render(request, 'firme.html', context)

    context = {
        'companies_count_by_city': companies_count_by_city,
    }
    return render(request, 'firme.html', context)


    def city_companies(request, city_name):
    file_paths = [settings.BASE_DIR / 'static' / 'Baza 2003.xlsx']
    all_data = pd.DataFrame()
    
    for fp in file_paths:
        data = pd.read_excel(fp)
        all_data = pd.concat([all_data, data], ignore_index=True)

    # Normalizacija Unicode karaktera za kolonu 'Opština'
    all_data['Opština'] = all_data['Opština'].astype(str).str.strip()
    all_data['Opština'] = all_data['Opština'].apply(lambda x: unicodedata.normalize('NFKC', x))

    # Dekodiranje i normalizacija za 'city_name'
    city_name = unquote(city_name).strip()
    city_name = unicodedata.normalize('NFKC', city_name)
    
    # Filtriranje podataka za odabranu opštinu koristeći case insensitive pretragu
    city_companies_data = all_data[all_data['Opština'].str.contains(city_name, case=False, na=False)]
    
    # Kreiranje liste kompanija za kontekst
    companies_list = city_companies_data.to_dict('records')

    paginator = Paginator(companies_list, 10)  # Prikaži 10 kompanija po stranici
    page_number = request.GET.get('page')
    page_obj = paginator.get_page(page_number)

    context = {
        'page_obj': page_obj,
        'city_name': city_name,
    }
    
    return render(request, 'city_companies.html', context)


    def last_cron_run(request):
    logger = logging.getLogger(__name__)
    last_updated = LastFetchTime.get_last_update_time()
    logger.info(f"Vrijeme posljednjeg ažuriranja: {last_updated}")
    context = {'last_updated': last_updated}
    return render(request, 'header.html', context)